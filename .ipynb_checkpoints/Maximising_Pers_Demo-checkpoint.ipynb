{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration: Maximising Persistence\n",
    "\n",
    "We demonstrate how we can find a spectral wavelet, parametrised by a basis of chebyshev polynomials, such that the total persistence of a graph's filtration is maximised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "import pickle\n",
    "\n",
    "from models import models\n",
    "from models import utils\n",
    "import numpy.polynomial.chebyshev as cheby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Model\n",
    "\n",
    "We consider a wavelet spanned by a degree 6 chebyshev polynomial, with coefficients normalised to 1. We truncate the barcode to the 25 most persistent intervals as a computational necessity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheby_degree = 6\n",
    "max_intervals = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MUTAG dataset has  188  graphs.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'MUTAG'\n",
    "\n",
    "graph_list = pickle.load(open('data_example/' + dataset_name + '/networkx_graphs.pkl', 'rb'))\n",
    "\n",
    "print('The ' + dataset_name + ' dataset has ', len(graph_list), ' graphs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "\n",
    "The ChebyshevWavelets pytorch module takes in a list of dictionaries, each dictionary representing the necessary data to compute spectral wavelets.\n",
    "\n",
    "The dictionary contains the following fields:\n",
    "- 'chebyshev': an intermediary matrix (num vertices) x (chebyshev degree) for computations; the vertex values of the filtration are given by the product of this matrix with the vector of chebyshev coefficients\n",
    "- 'simplex_tree': the Gudhi representation of a simplicial complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished initial processing\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(len(graph_list)):\n",
    "    \n",
    "    G = graph_list[i]\n",
    "    datum = dict()\n",
    "    L = nx.normalized_laplacian_matrix(G)\n",
    "    w, v = eigh(L.todense()) #computes eigenvalues w and eigenvectors\n",
    "    vandermonde = cheby.chebvander(w.flatten()-1, cheby_degree)\n",
    "    datum['chebyshev'] = torch.from_numpy(np.matmul(v**2, vandermonde[:, 1:])).float()\n",
    "\n",
    "    hks = np.matmul(v**2,  np.exp(-0.1*w)).flatten() #random initial filtration for the simplex_tree\n",
    "    st = utils.simplex_tree_constructor([list(e) for e in G.edges()])\n",
    "    datum['simplex_tree'] = utils.filtration_update(st, hks)\n",
    "    data.append(datum)\n",
    "print('Finished initial processing')\n",
    "del graph_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Design\n",
    "\n",
    "### Tenfolding\n",
    "\n",
    "We find a set of chebyshev coefficients that maximise the average $L^2$-persistence of the graph barcodes across the dataset\n",
    "\n",
    "We perform a ten-fold cross validation. In a ten-fold, we randomly partition the dataset into 10 portions. We perform the maximisation across 9 portions and then validate the learnt parameters on the remaining portion. We cycle through the ten portions so that each portion is the validation set once.\n",
    "\n",
    "Normally in machine learning practices, we conduct 10 ten-folds and average across all 100 validation measures, but in the interest of time we only perform one ten-fold in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(data)\n",
    "test_size = data_len // 10\n",
    "train_size = data_len - test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation using PyTorch tools\n",
    "We specify the batch size and the number of epochs. We use stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num points =  188  number of batches =  9  batch size =  20  test size  18\n"
     ]
    }
   ],
   "source": [
    "### training parameters #####\n",
    "batch_size = 20\n",
    "train_batches = np.ceil((data_len-test_size)/batch_size).astype(int)\n",
    "max_epoch = 25\n",
    "\n",
    "print('num points = ', data_len, ' number of batches = ', train_batches, ' batch size = ', batch_size, ' test size ', test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> fold  0\n",
      "0 [0.19873825, -0.38596588, -0.24803096, -0.21418718, 0.24152116, -0.2106402]\n",
      "train:  0.94126076\n",
      "test:  0.84419525\n",
      "5 [0.1942889, -0.40188175, -0.24247803, -0.19857855, 0.23643611, -0.21257612]\n",
      "train:  1.069699\n",
      "test:  0.9619286\n",
      "10 [0.18935049, -0.41823336, -0.23631474, -0.1813835, 0.23071557, -0.21411334]\n",
      "train:  1.2166607\n",
      "test:  1.0961475\n",
      "15 [0.18388596, -0.43480247, -0.22949494, -0.16251977, 0.22434296, -0.21524544]\n",
      "train:  1.3831999\n",
      "test:  1.2487042\n",
      "20 [0.1778703, -0.45137846, -0.22198732, -0.142042, 0.21725197, -0.21585584]\n",
      "train:  1.5694593\n",
      "test:  1.4189979\n",
      "> fold  1\n",
      "0 [0.19876146, -0.38589865, -0.24805996, -0.21424843, 0.24155203, -0.21060948]\n",
      "train:  0.9147942\n",
      "test:  1.0885763\n",
      "5 [0.1944401, -0.40147173, -0.24266678, -0.1989808, 0.2366406, -0.21239157]\n",
      "train:  1.0373375\n",
      "test:  1.2296659\n",
      "10 [0.18965077, -0.4174744, -0.23668957, -0.18219972, 0.23111643, -0.21378535]\n",
      "train:  1.1772486\n",
      "test:  1.3898085\n",
      "15 [0.18435724, -0.43371433, -0.23008308, -0.16378641, 0.22496746, -0.21479174]\n",
      "train:  1.3359221\n",
      "test:  1.5698606\n",
      "20 [0.17853396, -0.4499821, -0.2228156, -0.14381227, 0.21813336, -0.21530016]\n",
      "train:  1.513278\n",
      "test:  1.7699435\n",
      "> fold  2\n",
      "0 [0.1987444, -0.385954, -0.24803863, -0.2141778, 0.24153225, -0.21064404]\n",
      "train:  0.9343783\n",
      "test:  0.9088315\n",
      "5 [0.19432573, -0.40181443, -0.24252412, -0.19851853, 0.23650289, -0.21259852]\n",
      "train:  1.0625153\n",
      "test:  1.0277857\n",
      "10 [0.18941787, -0.41810802, -0.236399, -0.18127856, 0.23083119, -0.2141689]\n",
      "train:  1.2091308\n",
      "test:  1.1633893\n",
      "15 [0.18398291, -0.43463367, -0.22961599, -0.16234869, 0.22450593, -0.21533288]\n",
      "train:  1.37561\n",
      "test:  1.3160996\n",
      "20 [0.17799464, -0.45116988, -0.22214241, -0.14180234, 0.21745388, -0.21598314]\n",
      "train:  1.5619085\n",
      "test:  1.4861624\n",
      "> fold  3\n",
      "0 [0.19875024, -0.3859093, -0.24804592, -0.21423799, 0.24153885, -0.21064295]\n",
      "train:  0.92745686\n",
      "test:  0.970315\n",
      "5 [0.19437079, -0.40152913, -0.24258026, -0.198916, 0.23655659, -0.21260017]\n",
      "train:  1.0512812\n",
      "test:  1.1063447\n",
      "10 [0.1895206, -0.41756743, -0.23652704, -0.1820667, 0.23095869, -0.2141836]\n",
      "train:  1.192598\n",
      "test:  1.2613206\n",
      "15 [0.18416366, -0.43383753, -0.22984144, -0.16357222, 0.22472805, -0.21538252]\n",
      "train:  1.3528566\n",
      "test:  1.4357826\n",
      "20 [0.17827508, -0.45012343, -0.22249234, -0.14351566, 0.21780023, -0.21608949]\n",
      "train:  1.5318521\n",
      "test:  1.6302724\n",
      "> fold  4\n",
      "0 [0.19873545, -0.38599694, -0.24802752, -0.21413994, 0.24152075, -0.21063869]\n",
      "train:  0.94464517\n",
      "test:  0.81502956\n",
      "5 [0.19426475, -0.4020797, -0.24244794, -0.19826609, 0.23642516, -0.21256267]\n",
      "train:  1.07626\n",
      "test:  0.9199183\n",
      "10 [0.1892913, -0.4185993, -0.23624092, -0.18077622, 0.23067245, -0.21409345]\n",
      "train:  1.2270081\n",
      "test:  1.040059\n",
      "15 [0.18377648, -0.4353395, -0.22935818, -0.16157337, 0.22424906, -0.21521208]\n",
      "train:  1.3981404\n",
      "test:  1.1762136\n",
      "20 [0.17769448, -0.4520782, -0.22176774, -0.14073117, 0.21708587, -0.21579155]\n",
      "train:  1.5896322\n",
      "test:  1.3288964\n",
      "> fold  5\n",
      "0 [0.19874115, -0.3859402, -0.2480346, -0.21418865, 0.24153282, -0.21066543]\n",
      "train:  0.93790895\n",
      "test:  0.8748021\n",
      "5 [0.19430883, -0.40171725, -0.242503, -0.19859372, 0.23650828, -0.21274519]\n",
      "train:  1.065182\n",
      "test:  0.9971282\n",
      "10 [0.18939239, -0.41791052, -0.23636714, -0.18143488, 0.23084775, -0.21446142]\n",
      "train:  1.2105799\n",
      "test:  1.1371722\n",
      "15 [0.18395531, -0.43431255, -0.22958161, -0.16260761, 0.2245393, -0.2158096]\n",
      "train:  1.3753514\n",
      "test:  1.2959758\n",
      "20 [0.17797318, -0.45070866, -0.22211571, -0.14218508, 0.21751297, -0.216678]\n",
      "train:  1.5594419\n",
      "test:  1.473515\n",
      "> fold  6\n",
      "0 [0.19875503, -0.38589835, -0.24805188, -0.21424276, 0.24154231, -0.21064259]\n",
      "train:  0.9220707\n",
      "test:  1.0204656\n",
      "5 [0.19440064, -0.40146, -0.24261753, -0.1989503, 0.23657724, -0.21260542]\n",
      "train:  1.0451323\n",
      "test:  1.1594926\n",
      "10 [0.18957779, -0.41744396, -0.23659848, -0.18214083, 0.23100084, -0.21418555]\n",
      "train:  1.1855555\n",
      "test:  1.3177493\n",
      "15 [0.18425147, -0.43364906, -0.22995105, -0.16373935, 0.22479957, -0.21536702]\n",
      "train:  1.3443053\n",
      "test:  1.4977145\n",
      "20 [0.17839828, -0.4498597, -0.2226461, -0.1438025, 0.21790928, -0.21607654]\n",
      "train:  1.5213372\n",
      "test:  1.6991735\n",
      "> fold  7\n",
      "0 [0.19874103, -0.38597545, -0.24803445, -0.21419767, 0.2415274, -0.21059822]\n",
      "train:  0.93811035\n",
      "test:  0.8735847\n",
      "5 [0.1943068, -0.4018992, -0.2425003, -0.19867212, 0.23647463, -0.21237057]\n",
      "train:  1.0657358\n",
      "test:  0.9946755\n",
      "10 [0.18938768, -0.4182169, -0.23636116, -0.18160692, 0.23078254, -0.21379866]\n",
      "train:  1.2112044\n",
      "test:  1.134666\n",
      "15 [0.18394925, -0.43471807, -0.22957379, -0.16292934, 0.22444414, -0.21486095]\n",
      "train:  1.3753949\n",
      "test:  1.2961043\n",
      "20 [0.17796883, -0.4512072, -0.22211005, -0.14268245, 0.21739347, -0.21543907]\n",
      "train:  1.5585426\n",
      "test:  1.4777942\n",
      "> fold  8\n",
      "0 [0.19874924, -0.38592735, -0.24804468, -0.214213, 0.2415244, -0.21065424]\n",
      "train:  0.92873913\n",
      "test:  0.9599461\n",
      "5 [0.19436032, -0.40162706, -0.24256726, -0.19875014, 0.23647155, -0.21268941]\n",
      "train:  1.0543603\n",
      "test:  1.0888733\n",
      "10 [0.18949288, -0.41774315, -0.23649262, -0.18173955, 0.23079556, -0.2143574]\n",
      "train:  1.1978035\n",
      "test:  1.2360967\n",
      "15 [0.18411009, -0.43409666, -0.2297747, -0.16305856, 0.2244784, -0.21562803]\n",
      "train:  1.3606532\n",
      "test:  1.4014556\n",
      "20 [0.17818575, -0.4504705, -0.22238092, -0.14277694, 0.21745504, -0.21639335]\n",
      "train:  1.5428627\n",
      "test:  1.5849986\n",
      "> fold  9\n",
      "0 [0.1987534, -0.38590795, -0.2480499, -0.21423516, 0.24153888, -0.21064049]\n",
      "train:  0.92390424\n",
      "test:  1.0038235\n",
      "5 [0.19438964, -0.40151912, -0.24260382, -0.19890752, 0.2365535, -0.21258606]\n",
      "train:  1.0476242\n",
      "test:  1.1402903\n",
      "10 [0.18955517, -0.41754457, -0.23657025, -0.18208423, 0.23094702, -0.21414706]\n",
      "train:  1.188568\n",
      "test:  1.2967781\n",
      "15 [0.18421595, -0.4337813, -0.22990678, -0.16367091, 0.2247125, -0.21532188]\n",
      "train:  1.347807\n",
      "test:  1.4752119\n",
      "20 [0.1783488, -0.4500251, -0.22258432, -0.14372471, 0.21779013, -0.21600921]\n",
      "train:  1.5253451\n",
      "test:  1.6751883\n"
     ]
    }
   ],
   "source": [
    "####### torch random seeds #######\n",
    "shuffidx = list(range(data_len)) # data indexer\n",
    "\n",
    "torch.manual_seed(99)\n",
    "rng_state= torch.get_rng_state() #seed init to ensure same initial conditions for each training\n",
    "\n",
    "p_tracker = []\n",
    "tt_loss = []\n",
    "tn_loss = []\n",
    "\n",
    "for fold in range(10):\n",
    "    print ('> fold ', fold)\n",
    "\n",
    "    param_tracker = []\n",
    "    test_loss = []\n",
    "    train_loss = []\n",
    "\n",
    "    test_bottom = fold * test_size\n",
    "    test_top = (1+fold) * test_size\n",
    "    test_indices = shuffidx[test_bottom : test_top]\n",
    "    train_indices = shuffidx[0:test_bottom] + shuffidx[test_top :]\n",
    "\n",
    "    torch.set_rng_state(rng_state) #fix init state\n",
    "    barcodes = models.ChebyshevWavelets(cheby_degree = cheby_degree, max_intervals = max_intervals)\n",
    "    param_tracker.append(list(barcodes.cheby_params.detach().flatten().numpy()))\n",
    "\n",
    "    optimizer = optim.SGD(barcodes.parameters(), lr=1e-4, weight_decay = 0.0)\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        \n",
    "        barcodes.train()\n",
    "        np.random.shuffle(train_indices)\n",
    "        for b in range(train_batches):\n",
    "\n",
    "            train_indices_batch = train_indices[b*batch_size : (b+1)*batch_size ]\n",
    "            optimizer.zero_grad()\n",
    "            births, deaths = barcodes([data[i] for i in train_indices])\n",
    "            loss = -torch.sum((deaths - births)**2)/train_size\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "        barcodes.eval()\n",
    "        param_tracker.append(list(barcodes.cheby_params.detach().flatten().numpy()))\n",
    "\n",
    "\n",
    "        barcodes.eval()\n",
    "        b,d = barcodes([data[i] for i in train_indices])\n",
    "        tnl = torch.sum((d- b)**2)/train_size\n",
    "        b,d  = barcodes([data[i] for i in test_indices])\n",
    "        ttl = torch.sum((d- b)**2)/test_size\n",
    "        test_loss.append(ttl.detach().numpy())\n",
    "        train_loss.append(tnl.detach().numpy())\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(epoch, param_tracker[-1])\n",
    "            print('train: ', train_loss[-1])\n",
    "            print('test: ',test_loss[-1])\n",
    "\n",
    "    p_tracker.append(param_tracker)\n",
    "    tt_loss.append(test_loss)\n",
    "    tn_loss.append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array(0.84419525, dtype=float32),\n",
       "  array(0.8664122, dtype=float32),\n",
       "  array(0.88925713, dtype=float32),\n",
       "  array(0.91281945, dtype=float32),\n",
       "  array(0.9370448, dtype=float32),\n",
       "  array(0.9619286, dtype=float32),\n",
       "  array(0.9874242, dtype=float32),\n",
       "  array(1.0134817, dtype=float32),\n",
       "  array(1.0402309, dtype=float32),\n",
       "  array(1.0677447, dtype=float32),\n",
       "  array(1.0961475, dtype=float32),\n",
       "  array(1.1253159, dtype=float32),\n",
       "  array(1.1551636, dtype=float32),\n",
       "  array(1.1856643, dtype=float32),\n",
       "  array(1.2168387, dtype=float32),\n",
       "  array(1.2487042, dtype=float32),\n",
       "  array(1.2812785, dtype=float32),\n",
       "  array(1.314585, dtype=float32),\n",
       "  array(1.3486603, dtype=float32),\n",
       "  array(1.383465, dtype=float32),\n",
       "  array(1.4189979, dtype=float32),\n",
       "  array(1.455358, dtype=float32),\n",
       "  array(1.4924461, dtype=float32),\n",
       "  array(1.5301964, dtype=float32),\n",
       "  array(1.5685762, dtype=float32)],\n",
       " [array(1.0885763, dtype=float32),\n",
       "  array(1.1152787, dtype=float32),\n",
       "  array(1.142703, dtype=float32),\n",
       "  array(1.1709275, dtype=float32),\n",
       "  array(1.1999239, dtype=float32),\n",
       "  array(1.2296659, dtype=float32),\n",
       "  array(1.2601535, dtype=float32),\n",
       "  array(1.2913717, dtype=float32),\n",
       "  array(1.323366, dtype=float32),\n",
       "  array(1.3561637, dtype=float32),\n",
       "  array(1.3898085, dtype=float32),\n",
       "  array(1.4242886, dtype=float32),\n",
       "  array(1.4595457, dtype=float32),\n",
       "  array(1.4955367, dtype=float32),\n",
       "  array(1.5322778, dtype=float32),\n",
       "  array(1.5698606, dtype=float32),\n",
       "  array(1.6083034, dtype=float32),\n",
       "  array(1.6475228, dtype=float32),\n",
       "  array(1.6875358, dtype=float32),\n",
       "  array(1.7283657, dtype=float32),\n",
       "  array(1.7699435, dtype=float32),\n",
       "  array(1.8123673, dtype=float32),\n",
       "  array(1.855573, dtype=float32),\n",
       "  array(1.8995088, dtype=float32),\n",
       "  array(1.9441079, dtype=float32)],\n",
       " [array(0.9088315, dtype=float32),\n",
       "  array(0.93132603, dtype=float32),\n",
       "  array(0.954433, dtype=float32),\n",
       "  array(0.97823673, dtype=float32),\n",
       "  array(1.0026926, dtype=float32),\n",
       "  array(1.0277857, dtype=float32),\n",
       "  array(1.0535194, dtype=float32),\n",
       "  array(1.0799109, dtype=float32),\n",
       "  array(1.1069942, dtype=float32),\n",
       "  array(1.1348239, dtype=float32),\n",
       "  array(1.1633893, dtype=float32),\n",
       "  array(1.1926794, dtype=float32),\n",
       "  array(1.2226156, dtype=float32),\n",
       "  array(1.2531066, dtype=float32),\n",
       "  array(1.2842281, dtype=float32),\n",
       "  array(1.3160996, dtype=float32),\n",
       "  array(1.3487104, dtype=float32),\n",
       "  array(1.3820382, dtype=float32),\n",
       "  array(1.4160631, dtype=float32),\n",
       "  array(1.4507589, dtype=float32),\n",
       "  array(1.4861624, dtype=float32),\n",
       "  array(1.5223358, dtype=float32),\n",
       "  array(1.559169, dtype=float32),\n",
       "  array(1.5965897, dtype=float32),\n",
       "  array(1.6345918, dtype=float32)],\n",
       " [array(0.970315, dtype=float32),\n",
       "  array(0.9959872, dtype=float32),\n",
       "  array(1.02239, dtype=float32),\n",
       "  array(1.0496082, dtype=float32),\n",
       "  array(1.0776107, dtype=float32),\n",
       "  array(1.1063447, dtype=float32),\n",
       "  array(1.1358072, dtype=float32),\n",
       "  array(1.166014, dtype=float32),\n",
       "  array(1.196985, dtype=float32),\n",
       "  array(1.2287277, dtype=float32),\n",
       "  array(1.2613206, dtype=float32),\n",
       "  array(1.2947552, dtype=float32),\n",
       "  array(1.3289634, dtype=float32),\n",
       "  array(1.3637958, dtype=float32),\n",
       "  array(1.3993611, dtype=float32),\n",
       "  array(1.4357826, dtype=float32),\n",
       "  array(1.4730438, dtype=float32),\n",
       "  array(1.5111403, dtype=float32),\n",
       "  array(1.550058, dtype=float32),\n",
       "  array(1.5897789, dtype=float32),\n",
       "  array(1.6302724, dtype=float32),\n",
       "  array(1.6715758, dtype=float32),\n",
       "  array(1.7136251, dtype=float32),\n",
       "  array(1.7564485, dtype=float32),\n",
       "  array(1.7999731, dtype=float32)],\n",
       " [array(0.81502956, dtype=float32),\n",
       "  array(0.83481574, dtype=float32),\n",
       "  array(0.8551507, dtype=float32),\n",
       "  array(0.87613654, dtype=float32),\n",
       "  array(0.89772743, dtype=float32),\n",
       "  array(0.9199183, dtype=float32),\n",
       "  array(0.9426846, dtype=float32),\n",
       "  array(0.9660436, dtype=float32),\n",
       "  array(0.9900396, dtype=float32),\n",
       "  array(1.01469, dtype=float32),\n",
       "  array(1.040059, dtype=float32),\n",
       "  array(1.0660801, dtype=float32),\n",
       "  array(1.0927019, dtype=float32),\n",
       "  array(1.1198744, dtype=float32),\n",
       "  array(1.1476427, dtype=float32),\n",
       "  array(1.1762136, dtype=float32),\n",
       "  array(1.2054449, dtype=float32),\n",
       "  array(1.235351, dtype=float32),\n",
       "  array(1.2659117, dtype=float32),\n",
       "  array(1.2970989, dtype=float32),\n",
       "  array(1.3288964, dtype=float32),\n",
       "  array(1.3612326, dtype=float32),\n",
       "  array(1.394173, dtype=float32),\n",
       "  array(1.4277766, dtype=float32),\n",
       "  array(1.4619697, dtype=float32)],\n",
       " [array(0.8748021, dtype=float32),\n",
       "  array(0.89788127, dtype=float32),\n",
       "  array(0.9216175, dtype=float32),\n",
       "  array(0.9460983, dtype=float32),\n",
       "  array(0.9712718, dtype=float32),\n",
       "  array(0.9971282, dtype=float32),\n",
       "  array(1.0236797, dtype=float32),\n",
       "  array(1.0509129, dtype=float32),\n",
       "  array(1.078876, dtype=float32),\n",
       "  array(1.1076044, dtype=float32),\n",
       "  array(1.1371722, dtype=float32),\n",
       "  array(1.1674992, dtype=float32),\n",
       "  array(1.1985422, dtype=float32),\n",
       "  array(1.2302833, dtype=float32),\n",
       "  array(1.26273, dtype=float32),\n",
       "  array(1.2959758, dtype=float32),\n",
       "  array(1.3299855, dtype=float32),\n",
       "  array(1.3647649, dtype=float32),\n",
       "  array(1.4002806, dtype=float32),\n",
       "  array(1.4365488, dtype=float32),\n",
       "  array(1.473515, dtype=float32),\n",
       "  array(1.5114079, dtype=float32),\n",
       "  array(1.5501283, dtype=float32),\n",
       "  array(1.589604, dtype=float32),\n",
       "  array(1.6297771, dtype=float32)],\n",
       " [array(1.0204656, dtype=float32),\n",
       "  array(1.0467117, dtype=float32),\n",
       "  array(1.0736883, dtype=float32),\n",
       "  array(1.1015012, dtype=float32),\n",
       "  array(1.1301079, dtype=float32),\n",
       "  array(1.1594926, dtype=float32),\n",
       "  array(1.1896154, dtype=float32),\n",
       "  array(1.2203948, dtype=float32),\n",
       "  array(1.2519466, dtype=float32),\n",
       "  array(1.2843846, dtype=float32),\n",
       "  array(1.3177493, dtype=float32),\n",
       "  array(1.3521338, dtype=float32),\n",
       "  array(1.3873267, dtype=float32),\n",
       "  array(1.4233028, dtype=float32),\n",
       "  array(1.4600673, dtype=float32),\n",
       "  array(1.4977145, dtype=float32),\n",
       "  array(1.5362406, dtype=float32),\n",
       "  array(1.5756094, dtype=float32),\n",
       "  array(1.6159267, dtype=float32),\n",
       "  array(1.6571352, dtype=float32),\n",
       "  array(1.6991735, dtype=float32),\n",
       "  array(1.7421468, dtype=float32),\n",
       "  array(1.7860175, dtype=float32),\n",
       "  array(1.8307242, dtype=float32),\n",
       "  array(1.8762347, dtype=float32)],\n",
       " [array(0.8735847, dtype=float32),\n",
       "  array(0.8960919, dtype=float32),\n",
       "  array(0.9192521, dtype=float32),\n",
       "  array(0.94367146, dtype=float32),\n",
       "  array(0.96885586, dtype=float32),\n",
       "  array(0.9946755, dtype=float32),\n",
       "  array(1.0211141, dtype=float32),\n",
       "  array(1.0482202, dtype=float32),\n",
       "  array(1.0760952, dtype=float32),\n",
       "  array(1.1048406, dtype=float32),\n",
       "  array(1.134666, dtype=float32),\n",
       "  array(1.1653746, dtype=float32),\n",
       "  array(1.1968914, dtype=float32),\n",
       "  array(1.2291675, dtype=float32),\n",
       "  array(1.2622261, dtype=float32),\n",
       "  array(1.2961043, dtype=float32),\n",
       "  array(1.3307816, dtype=float32),\n",
       "  array(1.3662734, dtype=float32),\n",
       "  array(1.402604, dtype=float32),\n",
       "  array(1.4398103, dtype=float32),\n",
       "  array(1.4777942, dtype=float32),\n",
       "  array(1.5166106, dtype=float32),\n",
       "  array(1.5562115, dtype=float32),\n",
       "  array(1.5965565, dtype=float32),\n",
       "  array(1.6375945, dtype=float32)],\n",
       " [array(0.9599461, dtype=float32),\n",
       "  array(0.98420674, dtype=float32),\n",
       "  array(1.009192, dtype=float32),\n",
       "  array(1.0350341, dtype=float32),\n",
       "  array(1.06159, dtype=float32),\n",
       "  array(1.0888733, dtype=float32),\n",
       "  array(1.1168671, dtype=float32),\n",
       "  array(1.1455417, dtype=float32),\n",
       "  array(1.1749535, dtype=float32),\n",
       "  array(1.205126, dtype=float32),\n",
       "  array(1.2360967, dtype=float32),\n",
       "  array(1.2678393, dtype=float32),\n",
       "  array(1.3002645, dtype=float32),\n",
       "  array(1.333301, dtype=float32),\n",
       "  array(1.3669913, dtype=float32),\n",
       "  array(1.4014556, dtype=float32),\n",
       "  array(1.4366511, dtype=float32),\n",
       "  array(1.4725764, dtype=float32),\n",
       "  array(1.5093001, dtype=float32),\n",
       "  array(1.5467848, dtype=float32),\n",
       "  array(1.5849986, dtype=float32),\n",
       "  array(1.6240556, dtype=float32),\n",
       "  array(1.6638635, dtype=float32),\n",
       "  array(1.7044061, dtype=float32),\n",
       "  array(1.7455797, dtype=float32)],\n",
       " [array(1.0038235, dtype=float32),\n",
       "  array(1.0295581, dtype=float32),\n",
       "  array(1.0560355, dtype=float32),\n",
       "  array(1.083342, dtype=float32),\n",
       "  array(1.1114339, dtype=float32),\n",
       "  array(1.1402903, dtype=float32),\n",
       "  array(1.1698905, dtype=float32),\n",
       "  array(1.2002333, dtype=float32),\n",
       "  array(1.2314756, dtype=float32),\n",
       "  array(1.2636237, dtype=float32),\n",
       "  array(1.2967781, dtype=float32),\n",
       "  array(1.3308635, dtype=float32),\n",
       "  array(1.3657464, dtype=float32),\n",
       "  array(1.4014215, dtype=float32),\n",
       "  array(1.4378883, dtype=float32),\n",
       "  array(1.4752119, dtype=float32),\n",
       "  array(1.513429, dtype=float32),\n",
       "  array(1.5525235, dtype=float32),\n",
       "  array(1.5925326, dtype=float32),\n",
       "  array(1.6334634, dtype=float32),\n",
       "  array(1.6751883, dtype=float32),\n",
       "  array(1.717712, dtype=float32),\n",
       "  array(1.7609488, dtype=float32),\n",
       "  array(1.8050562, dtype=float32),\n",
       "  array(1.8499455, dtype=float32)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tt_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
